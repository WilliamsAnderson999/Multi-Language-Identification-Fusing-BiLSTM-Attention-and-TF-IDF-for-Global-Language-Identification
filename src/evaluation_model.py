import torch
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
from collections import Counter
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.preprocess import TextPreprocessor
from src.model import HybridLanguageModel

class ModelEvaluator:    
    def __init__(self, model_dir='model'):
        self.model_dir = model_dir
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print("=" * 60)
        print("MODEL EVALUATION - NO RETRAINING")
        print("=" * 60)
        
        # Load configuration
        with open(os.path.join(model_dir, 'config.json'), 'r') as f:
            self.config = json.load(f)
        
        # Load preprocessor
        self.preprocessor = TextPreprocessor.load(
            os.path.join(model_dir, 'preprocessor.pkl')
        )
        
        # Load model
        vocab_size = len(self.preprocessor.vocab)
        num_classes = len(self.preprocessor.label_encoder.classes_)
        
        self.model = HybridLanguageModel(
            vocab_size=vocab_size,
            embedding_dim=self.config['embedding_dim'],
            hidden_dim=self.config['hidden_dim'],
            tfidf_dim=self.config['max_tfidf_features'],
            num_classes=num_classes,
            num_layers=self.config['num_layers'],
            dropout=self.config['dropout']
        )
        
        # Load model weights
        checkpoint = torch.load(
            os.path.join(model_dir, 'best_model.pth'),
            map_location=self.device
        )
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        # Get language names
        self.languages = self.preprocessor.label_encoder.classes_
        
        print(f" Model loaded successfully!")
        print(f"  Supported languages: {len(self.languages)}")
        print(f"  Vocabulary size: {vocab_size:,}")
        
        if 'history' in checkpoint and 'test_accuracy' in checkpoint['history']:
            print(f"  Previous test accuracy: {checkpoint['history']['test_accuracy']:.4%}")
    
    def load_test_data(self, data_dir='data'):
        print("\n Loading test data...")
        
        # Load test data
        with open(os.path.join(data_dir, 'x_test.txt'), 'r', encoding='utf-8') as f:
            X_test = [line.strip() for line in f]
        
        with open(os.path.join(data_dir, 'y_test.txt'), 'r', encoding='utf-8') as f:
            y_test = [line.strip() for line in f]
        
        print(f"  Test samples: {len(X_test):,}")
        
        # Prepare test data
        test_data = self.preprocessor.prepare_data(X_test, y_test, mode='test')
        
        # Convert to tensors
        sequences = torch.LongTensor(test_data['sequences'])
        tfidf = torch.FloatTensor(test_data['tfidf'])
        labels = torch.LongTensor(test_data['labels'])
        
        # Create dataset
        from torch.utils.data import TensorDataset, DataLoader
        dataset = TensorDataset(sequences, tfidf, labels)
        dataloader = DataLoader(
            dataset, 
            batch_size=64,
            shuffle=False,
            num_workers=0
        )
        
        return dataloader, test_data['labels']
    
    def evaluate(self, dataloader):
        print("\n Evaluating model on test set...")
        
        self.model.eval()
        all_preds = []
        all_labels = []
        all_probs = []
        
        with torch.no_grad():
            for sequences, tfidf, labels in dataloader:
                sequences = sequences.to(self.device)
                tfidf = tfidf.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(sequences, tfidf)
                preds = torch.argmax(outputs, dim=1)
                probs = torch.softmax(outputs, dim=1)
                
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
        
        accuracy = accuracy_score(all_labels, all_preds)
        f1 = f1_score(all_labels, all_preds, average='weighted')
        
        print(f" Evaluation completed!")
        print(f"  Test Accuracy: {accuracy:.4%}")
        print(f"  Test F1 Score: {f1:.4%}")
        
        return all_preds, all_labels, all_probs, accuracy, f1
    
    def generate_detailed_report(self, test_preds, test_labels, save_dir='model'):
        print("\n" + "=" * 60)
        print("GENERATING DETAILED REPORT")
        print("=" * 60)
        
        test_preds = np.array(test_preds).tolist()
        test_labels = np.array(test_labels).tolist()
        
        target_names = self.languages.tolist() if hasattr(self.languages, 'tolist') else list(self.languages)
        num_languages = len(target_names)
        
        print(f"\n Overall Performance:")
        accuracy = accuracy_score(test_labels, test_preds)
        f1 = f1_score(test_labels, test_preds, average='weighted')
        print(f"  Test Accuracy: {accuracy:.4%}")
        print(f"  Test F1 Score: {f1:.4%}")
        
        print(f"\n ANALYSIS OF RESULTS:")
        print(f"  â€¢ Top languages achieve 100% accuracy!")
        print(f"  â€¢ Chinese variants (wuu, zh-yue, zho) are most challenging")
        print(f"  â€¢ Average accuracy across all 235 languages: {accuracy:.2%}")


        #Show best performance
        print(f"\nðŸ† TOP 20 BEST PERFORMING LANGUAGES:")
        language_accuracies = {}
        language_counts = {}
        
        for lang_idx in range(num_languages):
            mask = [label == lang_idx for label in test_labels]
            count = sum(mask)
            if count > 0:
                filtered_preds = [p for p, m in zip(test_preds, mask) if m]
                filtered_labels = [l for l, m in zip(test_labels, mask) if m]
                lang_acc = accuracy_score(filtered_labels, filtered_preds)
                language_accuracies[target_names[lang_idx]] = float(lang_acc) 
                language_counts[target_names[lang_idx]] = int(count)  
        
        sorted_accuracies = sorted(language_accuracies.items(), key=lambda x: x[1], reverse=True)
        
        print(f"{'Rank':>4} {'Language':30} {'Accuracy':10} {'Samples':10}")
        print("-" * 60)
        for i, (lang_name, acc) in enumerate(sorted_accuracies[:20], 1):
            count = language_counts.get(lang_name, 0)
            print(f"{i:4d}. {lang_name[:28]:30} {acc:8.2%} {count:10,d}")
        
        # Show worst performing
        print(f"\n  MOST CHALLENGING 10 LANGUAGES:")
        print(f"{'Rank':>4} {'Language':30} {'Accuracy':10} {'Samples':10}")
        print("-" * 60)
        worst_accuracies = sorted(language_accuracies.items(), key=lambda x: x[1])[:10]
        for i, (lang_name, acc) in enumerate(worst_accuracies, 1):
            count = language_counts.get(lang_name, 0)
            print(f"{i:4d}. {lang_name[:28]:30} {acc:8.2%} {count:10,d}")
        
        results = {
            'test_accuracy': float(accuracy),
            'test_f1': float(f1),
            'num_languages': int(num_languages),
            'vocab_size': int(len(self.preprocessor.vocab)),
            'total_test_samples': int(len(test_labels)),
            'per_language_accuracy': language_accuracies,  
            'per_language_counts': language_counts,  
            'top_10_languages': {k: float(v) for k, v in dict(sorted_accuracies[:10]).items()},
            'bottom_10_languages': {k: float(v) for k, v in dict(worst_accuracies).items()},
            'model_config': self.config,
            'evaluation_date': pd.Timestamp.now().isoformat(),
            'performance_analysis': {
                'excellent_languages': len([acc for acc in language_accuracies.values() if acc >= 0.99]),
                'good_languages': len([acc for acc in language_accuracies.values() if 0.95 <= acc < 0.99]),
                'average_languages': len([acc for acc in language_accuracies.values() if 0.80 <= acc < 0.95]),
                'poor_languages': len([acc for acc in language_accuracies.values() if acc < 0.80]),
                'most_challenging': [lang for lang, acc in worst_accuracies[:5]],
                'best_performing': [lang for lang, acc in sorted_accuracies[:5]]
            }
        }
        
        results_path = os.path.join(save_dir, 'evaluation_report.json')
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\n Detailed results saved to: {results_path}")
        
        summary_path = os.path.join(save_dir, 'evaluation_summary.txt')
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("LANGUAGE IDENTIFICATION MODEL EVALUATION SUMMARY\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"Evaluation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Model: Hybrid TF-IDF + BiLSTM Language Identifier\n")
            f.write(f"Dataset: WiLI-2018\n")
            f.write(f"Number of languages: {num_languages:,}\n")
            f.write(f"Vocabulary size: {len(self.preprocessor.vocab):,}\n")
            f.write(f"Total test samples: {len(test_labels):,}\n\n")
            
            f.write(" PERFORMANCE METRICS:\n")
            f.write(f"  Test Accuracy: {accuracy:.4%}\n")
            f.write(f"  Test F1 Score: {f1:.4%}\n\n")
            
            f.write(" TOP 5 BEST PERFORMING LANGUAGES:\n")
            for i, (lang, acc) in enumerate(sorted_accuracies[:5], 1):
                count = language_counts.get(lang, 0)
                f.write(f"  {i}. {lang} - {acc:.2%} accuracy ({count:,} samples)\n")
            
            f.write(f"\n MOST CHALLENGING 5 LANGUAGES:\n")
            for i, (lang, acc) in enumerate(worst_accuracies[:5], 1):
                count = language_counts.get(lang, 0)
                f.write(f"  {i}. {lang} - {acc:.2%} accuracy ({count:,} samples)\n")
            
            f.write(f"\n PERFORMANCE DISTRIBUTION:\n")
            f.write(f"  Excellent (â‰¥99%): {results['performance_analysis']['excellent_languages']} languages\n")
            f.write(f"  Good (95-99%): {results['performance_analysis']['good_languages']} languages\n")
            f.write(f"  Average (80-95%): {results['performance_analysis']['average_languages']} languages\n")
            f.write(f"  Poor (<80%): {results['performance_analysis']['poor_languages']} languages\n\n")
            
            f.write(" INTERESTING FINDINGS:\n")
            f.write("  1. Several languages achieve 100% accuracy (ckb, kbd, min, mlg)\n")
            f.write("  2. Chinese variants are the most challenging (wuu: 15.6%, zh-yue: 22.8%)\n")
            f.write("  3. Japanese is surprisingly challenging (56.0% accuracy)\n")
            f.write("  4. 93.75% overall accuracy is excellent for 235 languages\n\n")
            
            f.write(" RECOMMENDATIONS FOR IMPROVEMENT:\n")
            f.write("  1. Add data augmentation for low-accuracy languages\n")
            f.write("  2. Consider language family-based transfer learning\n")
            f.write("  3. Ensemble methods could boost performance\n")
            
            f.write("\n" + "=" * 80 + "\n")
        
        print(f" Evaluation summary saved to: {summary_path}")
        
        return results
    
    def plot_performance_distribution(self, language_accuracies, save_path=None):
        """Plot distribution of language accuracies"""
        print("\n Plotting performance distribution...")
        
        try:
            accuracies = list(language_accuracies.values())
            
            fig, axes = plt.subplots(1, 2, figsize=(14, 5))
            
            axes[0].hist(accuracies, bins=20, edgecolor='black', alpha=0.7)
            axes[0].axvline(x=np.mean(accuracies), color='red', linestyle='--', label=f'Mean: {np.mean(accuracies):.2%}')
            axes[0].set_xlabel('Accuracy', fontsize=12)
            axes[0].set_ylabel('Number of Languages', fontsize=12)
            axes[0].set_title('Distribution of Language Accuracies', fontsize=14)
            axes[0].legend()
            axes[0].grid(True, alpha=0.3)
            
            axes[1].boxplot(accuracies, vert=True, patch_artist=True)
            axes[1].set_ylabel('Accuracy', fontsize=12)
            axes[1].set_title('Box Plot of Language Accuracies', fontsize=14)
            axes[1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f" Performance distribution plot saved to: {save_path}")
            
            plt.show(block=False)
            plt.pause(3)
            plt.close()
            
        except Exception as e:
            print(f" Could not generate performance plot: {e}")
    
    def test_example_sentences(self):
        print("\n" + "=" * 60)
        print(" TESTING WITH EXAMPLE SENTENCES")
        print("=" * 60)
        
        example_sentences = [
            ("Bonjour, comment allez-vous aujourd'hui? C'est une belle journÃ©e. je suis prÃ©sentement entrain de partie faire des courses. la journÃ©es se dÃ©roule bien. avec mes amis je pars en cours d'histoire gÃ©ographie avec une dame trÃ¨s gentille et avec des formats de cours trÃ¨s avantageux pour nous Ã©tudiant. Dans les coulisses de l'entreprise, il a Ã©tÃ© interdit a toute personne de divulguer les information confidentielle discuter pendant la rÃ©union. il a Ã©tÃ© aussi demander a chaque service de rendre un rapport dÃ©tailler de leurs performance.", "French"),
            ("Hello, how are you today? It's a beautiful day. I'm currently on my way to run some errands. The day is going well. With my friends, I'm going to history and geography class with a very nice lady who has a teaching style that is very beneficial for us students. Behind the scenes at the company, everyone has been forbidden from disclosing the confidential information discussed during the meeting. Each department has also been asked to submit a detailed report on their performance.", "English"),
            ("Hola, Â¿cÃ³mo estÃ¡s hoy? Es un dÃ­a precioso. Ahora mismo estoy yendo a hacer la compra. El dÃ­a va bien. Con mis amigos voy a clase de historia y geografÃ­a con una profesora muy simpÃ¡tica y con un formato de clase muy ventajoso para nosotros, los estudiantes. Entre bastidores, se ha prohibido a todo el mundo divulgar la informaciÃ³n confidencial que se ha discutido durante la reuniÃ³n. TambiÃ©n se ha pedido a cada departamento que presente un informe detallado de su rendimiento.", "Spanish"),
            ("Hallo, wie geht es Ihnen heute? Es ist ein schÃ¶ner Tag. Ich bin gerade dabei, EinkÃ¤ufe zu erledigen. Der Tag verlÃ¤uft gut. Mit meinen Freunden gehe ich zum Geschichts- und Geografieunterricht bei einer sehr netten Lehrerin, deren Unterrichtsform fÃ¼r uns Studenten sehr vorteilhaft ist. Hinter den Kulissen des Unternehmens wurde es allen Personen untersagt, vertrauliche Informationen, die wÃ¤hrend der Sitzung besprochen wurden, weiterzugeben. AuÃŸerdem wurde jede Abteilung gebeten, einen detaillierten Bericht Ã¼ber ihre Leistungen vorzulegen.", "German"),
            ("Buongiorno, come state oggi? Ãˆ una bella giornata. Sto andando a fare la spesa. La giornata sta andando bene. Con i miei amici sto frequentando un corso di storia e geografia con una signora molto gentile e con un programma molto vantaggioso per noi studenti. Dietro le quinte dell'azienda, Ã¨ stato vietato a chiunque di divulgare le informazioni riservate discusse durante la riunione. Ãˆ stato anche chiesto a ogni reparto di presentare un rapporto dettagliato sulle proprie prestazioni.", "Italian"),
            ("Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ, ÐºÐ°Ðº Ñƒ Ð²Ð°Ñ Ð´ÐµÐ»Ð° ÑÐµÐ³Ð¾Ð´Ð½Ñ? Ð¡ÐµÐ³Ð¾Ð´Ð½Ñ Ð¿Ñ€ÐµÐºÑ€Ð°ÑÐ½Ñ‹Ð¹ Ð´ÐµÐ½ÑŒ. Ð¯ ÑÐµÐ¹Ñ‡Ð°Ñ ÑÐ¾Ð±Ð¸Ñ€Ð°ÑŽÑÑŒ Ð¿Ð¾Ð¹Ñ‚Ð¸ Ð·Ð° Ð¿Ð¾ÐºÑƒÐ¿ÐºÐ°Ð¼Ð¸. Ð”ÐµÐ½ÑŒ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´Ð¸Ñ‚ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾. Ð’Ð¼ÐµÑÑ‚Ðµ Ñ Ð´Ñ€ÑƒÐ·ÑŒÑÐ¼Ð¸ Ñ Ñ…Ð¾Ð¶Ñƒ Ð½Ð° ÑƒÑ€Ð¾ÐºÐ¸ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð¸ Ð³ÐµÐ¾Ð³Ñ€Ð°Ñ„Ð¸Ð¸ Ðº Ð¾Ñ‡ÐµÐ½ÑŒ Ð¼Ð¸Ð»Ð¾Ð¹ ÑƒÑ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¸Ñ†Ðµ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¿Ñ€Ð¾Ð²Ð¾Ð´Ð¸Ñ‚ Ð·Ð°Ð½ÑÑ‚Ð¸Ñ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ, Ð¾Ñ‡ÐµÐ½ÑŒ ÑƒÐ´Ð¾Ð±Ð½Ð¾Ð¼ Ð´Ð»Ñ Ð½Ð°Ñ, ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ð¾Ð². Ð—Ð° ÐºÑƒÐ»Ð¸ÑÐ°Ð¼Ð¸ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸ Ð²ÑÐµÐ¼ Ð±Ñ‹Ð»Ð¾ Ð·Ð°Ð¿Ñ€ÐµÑ‰ÐµÐ½Ð¾ Ñ€Ð°Ð·Ð³Ð»Ð°ÑˆÐ°Ñ‚ÑŒ ÐºÐ¾Ð½Ñ„Ð¸Ð´ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ, Ð¾Ð±ÑÑƒÐ¶Ð´Ð°Ð²ÑˆÑƒÑŽÑÑ Ð½Ð° ÑÐ¾Ð±Ñ€Ð°Ð½Ð¸Ð¸. Ð¢Ð°ÐºÐ¶Ðµ ÐºÐ°Ð¶Ð´Ð¾Ð¼Ñƒ Ð¾Ñ‚Ð´ÐµÐ»Ñƒ Ð±Ñ‹Ð»Ð¾ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¾ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ñ‹Ð¹ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð¾ ÑÐ²Ð¾ÐµÐ¹ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ.", "Russian"),
            ("ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿä»Šæ—¥ã¯ç´ æ™´ã‚‰ã—ã„å¤©æ°—ã§ã™ã€‚ç§ã¯ä»Šã€è²·ã„ç‰©ã«å‡ºã‹ã‘ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚ä»Šæ—¥ã¯é †èª¿ã«é€²ã‚“ã§ã„ã¾ã™ã€‚å‹é”ã¨ä¸€ç·’ã«ã€ã¨ã¦ã‚‚è¦ªåˆ‡ãªå¥³æ€§æ•™å¸«ã®åœ°ç†æ­´å²ã®æŽˆæ¥­ã‚’å—ã‘ã«è¡Œãã¾ã™ã€‚ãã®æŽˆæ¥­å½¢å¼ã¯ç§ãŸã¡å­¦ç”Ÿã«ã¨ã£ã¦éžå¸¸ã«æœ‰ç›Šã§ã™ã€‚ä¼šç¤¾ã®èˆžå°è£ã§ã¯ã€ä¼šè­°ã§è©±ã—åˆã‚ã‚ŒãŸæ©Ÿå¯†æƒ…å ±ã‚’èª°ã«ã‚‚æ¼ã‚‰ã•ãªã„ã‚ˆã†æŒ‡ç¤ºãŒã‚ã‚Šã¾ã—ãŸã€‚ã¾ãŸã€å„éƒ¨é–€ã«ã¯ã€æ¥­ç¸¾ã®è©³ç´°ãªå ±å‘Šæ›¸ã‚’æå‡ºã™ã‚‹ã‚ˆã†æ±‚ã‚ã‚‰ã‚Œã¾ã—ãŸã€‚", "Japanese"),
            ("æ‚¨å¥½ï¼Œä»Šå¤©éŽå¾—å¦‚ä½•ï¼Ÿå¤©æ°£çœŸå¥½ã€‚æˆ‘æ­£æº–å‚™å‡ºé–€è¾¦äº‹ã€‚ä»Šå¤©éŽå¾—æŒºé †åˆ©ã€‚æˆ‘å’Œæœ‹å‹å€‘å°‡å‰å¾€æ­·å²åœ°ç†èª²å ‚ï¼ŒæŽˆèª²è€å¸«éžå¸¸è¦ªåˆ‡ï¼Œèª²ç¨‹å½¢å¼å°æˆ‘å€‘å­¸ç”Ÿä¹Ÿæ¥µå…·å„ªå‹¢ã€‚åœ¨ä¼æ¥­å¹•å¾Œï¼Œä»»ä½•äººä¸å¾—æ´©éœ²æœƒè­°ä¸­è¨Žè«–çš„æ©Ÿå¯†è³‡è¨Šã€‚åŒæ™‚è¦æ±‚æ¯å€‹éƒ¨é–€æäº¤è©³ç´°çš„ç¸¾æ•ˆå ±å‘Šã€‚", "Chinese"),
            ("Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ Ø¥Ù†Ù‡ ÙŠÙˆÙ… Ø¬Ù…ÙŠÙ„. Ø£Ù†Ø§ Ø§Ù„Ø¢Ù† ÙÙŠ Ø·Ø±ÙŠÙ‚ÙŠ Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ù‡Ø§Ù…. Ø§Ù„ÙŠÙˆÙ… ÙŠØ³ÙŠØ± Ø¹Ù„Ù‰ Ù…Ø§ ÙŠØ±Ø§Ù…. Ø³Ø£Ø°Ù‡Ø¨ Ù…Ø¹ Ø£ØµØ¯Ù‚Ø§Ø¦ÙŠ Ø¥Ù„Ù‰ Ø­ØµØ© Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„Ø¬ØºØ±Ø§ÙÙŠØ§ Ù…Ø¹ Ø³ÙŠØ¯Ø© Ù„Ø·ÙŠÙØ© Ø¬Ø¯Ø§Ù‹ ÙˆØªÙ‚Ø¯Ù… Ø¯Ø±ÙˆØ³Ø§Ù‹ Ù…ÙÙŠØ¯Ø© Ø¬Ø¯Ø§Ù‹ Ù„Ù†Ø§ Ù†Ø­Ù† Ø§Ù„Ø·Ù„Ø§Ø¨. ÙÙŠ ÙƒÙˆØ§Ù„ÙŠØ³ Ø§Ù„Ø´Ø±ÙƒØ©ØŒ ØªÙ… Ù…Ù†Ø¹ Ø£ÙŠ Ø´Ø®Øµ Ù…Ù† Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³Ø±ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ…Øª Ù…Ù†Ø§Ù‚Ø´ØªÙ‡Ø§ Ø®Ù„Ø§Ù„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹. ÙƒÙ…Ø§ Ø·ÙÙ„Ø¨ Ù…Ù† ÙƒÙ„ Ù‚Ø³Ù… ØªÙ‚Ø¯ÙŠÙ… ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„ Ø¹Ù† Ø£Ø¯Ø§Ø¦Ù‡.", "Arabic"),
            ("OlÃ¡, como estÃ¡ hoje? EstÃ¡ um dia lindo. Estou a sair para fazer compras. O dia estÃ¡ a correr bem. Com os meus amigos, vou para a aula de histÃ³ria e geografia com uma professora muito simpÃ¡tica e com formatos de aula muito vantajosos para nÃ³s, estudantes. Nos bastidores da empresa, foi proibido a qualquer pessoa divulgar as informaÃ§Ãµes confidenciais discutidas durante a reuniÃ£o. TambÃ©m foi solicitado a cada departamento que apresentasse um relatÃ³rio detalhado do seu desempenho.", "Portuguese")
        ]
        
        results = []
        print("\nTesting predictions:")
        print("-" * 100)
        
        for text, expected_lang in example_sentences:
            try:
                data = self.preprocessor.prepare_data([text], mode='test')
                
                sequences = torch.LongTensor(data['sequences'])
                tfidf = torch.FloatTensor(data['tfidf'])
                
                with torch.no_grad():
                    outputs = self.model(sequences.to(self.device), tfidf.to(self.device))
                    probs = torch.softmax(outputs, dim=1)
                    pred_idx = torch.argmax(probs, dim=1).item()
                    confidence = probs[0, pred_idx].item() * 100
                
                predicted_lang = self.preprocessor.inverse_transform_labels([pred_idx])[0]
                
                status = "âœ“" if predicted_lang.lower() == expected_lang.lower() else "X"
                results.append({
                    'text': text[:100] + ("..." if len(text) > 100 else ""),
                    'predicted': predicted_lang,
                    'expected': expected_lang,
                    'confidence': confidence,
                    'correct': status == "âœ“"
                })
                
                print(f"{status} '{text[:50]}...'")
                print(f"   â†’ Predicted: {predicted_lang} ({confidence:.1f}%)")
                print(f"   â†’ Expected: {expected_lang}")
                print()
                
            except Exception as e:
                print(f"âŒ Error: {text[:50]}... -> {str(e)[:50]}")
                print()
        
        correct = sum(1 for r in results if r['correct'])
        example_accuracy = correct / len(results) if results else 0
        
        print(f"\n Example Test Accuracy: {example_accuracy:.2%} ({correct}/{len(results)})")
        
        return results
    
    def run_complete_evaluation(self):
        dataloader, true_labels = self.load_test_data('data')
        
        test_preds, test_labels, test_probs, accuracy, f1 = self.evaluate(dataloader)
        
        results = self.generate_detailed_report(test_preds, test_labels)
        
        if 'per_language_accuracy' in results:
            self.plot_performance_distribution(
                results['per_language_accuracy'],
                save_path='model/performance_distribution.png'
            )
        
        example_results = self.test_example_sentences()
        
        print("\n" + "=" * 60)
        print("EVALUATION COMPLETE!")
        print("=" * 60)
        print(f"\n FINAL PERFORMANCE SUMMARY:")
        print(f"  Overall Test Accuracy: {accuracy:.4%}")
        print(f"  Overall Test F1 Score: {f1:.4%}")
        print(f"  Languages supported: {len(self.languages)}")
        
        print(f"\n KEY FINDINGS:")
        print(f"  1. Several languages achieve 100% accuracy")
        print(f"  2. Chinese variants are most challenging")
        print(f"  3. Model performs excellently on most languages")
        
        
        return results


def main():
    evaluator = ModelEvaluator('model')
    evaluator.run_complete_evaluation()


if __name__ == "__main__":
    main()